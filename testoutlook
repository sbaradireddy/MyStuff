

import msal
import requests
import boto3
import base64
import os

def lambda_handler(event, context):
    scopes = ['https://graph.microsoft.com/.default']

    # Azure AD App details
    CLIENT_ID = 'jhvcjhvc'
    CLIENT_SECRET = 'jhvfgjhv'
    TENANT_ID = 'jhvjhvf'
    
    # AWS S3 bucket details
    S3_BUCKET = "pr-home-datascience"

    # Set up MSAL
    authority = f'https://login.microsoftonline.com/{TENANT_ID}'
    app = msal.ConfidentialClientApplication(CLIENT_ID, authority=authority, client_credential=CLIENT_SECRET)

    # Authenticate and get token
    result = app.acquire_token_for_client(scopes=scopes)
    access_token = result['access_token']

    # Base Microsoft Graph URL for fetching emails from the inbox
    graph_url = 'https://graph.microsoft.com/v1.0/users/HomeDSAWS@plymouthrock.com/mailFolders/inbox/messages'
    
    headers = {
        'Authorization': f'Bearer {access_token}',
        'Accept': 'application/json'
    }

    s3 = boto3.client('s3')

    # Function to process emails (write to S3)
    def process_emails(emails):
        for email in emails:
            email_id = email['id']
            split_email_id = email_id.split('-')
            
            if len(split_email_id) > 2:
                folder_name = split_email_id[2].rstrip('/') + '/'
            else:
                folder_name = 'outlook/'

            try:
                # Check if the folder already exists in the S3 bucket
                response = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=folder_name, Delimiter='/')
                if 'Contents' in response or 'CommonPrefixes' in response:
                    print(f"Email {email_id} already exists in bucket")
                    continue
                else:
                    s3.put_object(Bucket=S3_BUCKET, Key=folder_name)
                    print(f"Folder {folder_name} created in Bucket")
                
                # Save email content to S3
                email_content = email['subject'] + email['body']['content']
                filename = '/tmp/body.HTML'
                with open(filename, 'w', encoding='utf-8') as file:
                    file.write(email_content)

                s3_key = f"{folder_name}{os.path.basename(filename)}"
                s3.upload_file(filename, S3_BUCKET, s3_key)
                print(f"Uploaded email content as {filename} to S3")

                # Fetch and save email attachments
                attachments_url = f'https://graph.microsoft.com/v1.0/users/HomeDSAWS@plymouthrock.com/messages/{email_id}/attachments'
                response1 = requests.get(attachments_url, headers=headers)
                attachments = response1.json().get('value', [])
                
                for attachment in attachments:
                    if '@odata.type' in attachment and attachment['@odata.type'] == '#microsoft.graph.fileAttachment':
                        attachment_content = base64.b64decode(attachment['contentBytes'])
                        attachment_name = '/tmp/' + attachment['name']  # Save attachment in /tmp
                        
                        # Write attachment to file and upload to S3
                        with open(attachment_name, 'wb') as f:
                            f.write(attachment_content)

                        with open(attachment_name, 'rb') as f:
                            s3.upload_fileobj(f, S3_BUCKET, f"{folder_name}{os.path.basename(attachment_name)}")
                        print(f"Uploaded {attachment_name} to S3")
            
            except Exception as e:
                print('An error occurred', e)

    # Fetch emails and paginate if necessary
    def fetch_emails(graph_url):
        while graph_url:
            response = requests.get(graph_url, headers=headers)
            emails = response.json().get('value', [])
            process_emails(emails)
            
            # Check if there is another page of results
            graph_url = response.json().get('@odata.nextLink')

    # Start fetching emails
    fetch_emails(graph_url)










import msal
import requests
import boto3
import base64
import os

def lambda_handler(event, context):
    scopes = ['https://graph.microsoft.com/.default']

    # Azure AD App details
    CLIENT_ID = 'jhvcjhvc'
    CLIENT_SECRET = 'jhvfgjhv'
    TENANT_ID = 'jhvjhvf'
	
    # AWS S3 bucket details
    S3_BUCKET = "pr-home-datascience"

    # Set up MSAL
    authority = f'https://login.microsoftonline.com/{TENANT_ID}'
    app = msal.ConfidentialClientApplication(CLIENT_ID, authority=authority, client_credential=CLIENT_SECRET)

    # Authenticate and get token
    result = app.acquire_token_for_client(scopes=scopes)
    access_token = result['access_token']

    graph_url = 'https://graph.microsoft.com/v1.0/users/HomeDSAWS@plymouthrock.com/mailFolders/inbox/messages?$filter=hasAttachments eq true'

    headers = {
        'Authorization': f'Bearer {access_token}',
        'Accept': 'application/json'
    }
    
    s3 = boto3.client('s3')

    # Retrieve emails
    response = requests.get(graph_url, headers=headers)
    emails_with_attachments = response.json().get('value', [])

    for email in emails_with_attachments:
        email_id = email['id']
        split_email_id = email_id.split('-')
        
        if len(split_email_id) > 2:
            folder_name = split_email_id[2].rstrip('/') + '/'
        else:
            folder_name = 'outlook'

        try:
            response = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=folder_name, Delimiter='/')
            if 'Contents' in response or 'CommonPrefixes' in response:
                print('Email already exists in bucket')
                continue
            else:
                s3.put_object(Bucket=S3_BUCKET, Key=folder_name)
                print(f"Folder {folder_name} created in Bucket")
                
                email_content = email['subject'] + email['body']['content']
                
                # Create a txt file with email subject and body in /tmp
                filename = '/tmp/body.HTML'
                with open(filename, 'w', encoding='utf-8') as file:
                    file.write(email_content)
                    
                s3_key = f"{folder_name}{os.path.basename(filename)}"
                s3.upload_file(filename, S3_BUCKET, s3_key)
                print(f"Uploaded email content as {filename} to S3")
        
        except Exception as e:
            print('An error occurred', e)
        
        # Fetch attachments
        attachments_url = f'https://graph.microsoft.com/v1.0/users/HomeDSAWS@plymouthrock.com/messages/{email_id}/attachments'
        response1 = requests.get(attachments_url, headers=headers)
        attachments = response1.json().get('value', [])
        
        for attachment in attachments:
            if '@odata.type' in attachment and attachment['@odata.type'] == '#microsoft.graph.fileAttachment':
                attachment_content = base64.b64decode(attachment['contentBytes'])
                attachment_name = '/tmp/' + attachment['name']  # Use /tmp for Lambda's writable space
                
                # Save attachment in /tmp directory
                with open(attachment_name, 'wb') as f:
                    f.write(attachment_content)

                # Upload to S3
                with open(attachment_name, 'rb') as f:
                    s3.upload_fileobj(f, S3_BUCKET, f"{folder_name}{os.path.basename(attachment_name)}")
                print(f"Uploaded {attachment_name} to S3")



###########################################
Test Event Name
test

Response
{
  "errorType": "Runtime.ExitError",
  "errorMessage": "RequestId: 022cd06e-8ef4-4cc9-93ad-6c1f0cfae97f Error: Runtime exited with error: signal: killed"
}

Function Logs
START RequestId: 022cd06e-8ef4-4cc9-93ad-6c1f0cfae97f Version: $LATEST
emails count -  9
Folder AAMkAGU4ZWRkMGI3LWQ0NDctNGE3ZS1iN2QyLTlmODg2YmFlMzMyNwBGAAAAAABVVKFeqdKSSYHCm2TtRWO0BwA1Qe0bgT4cTptlxQT1mdz3AAAAAAEMAAA1Qe0bgT4cTptlxQT1mdz3AAAG-Q3aAAA= created in Bucket
Uploaded email content as /tmp/body.HTML to S3
AAMkAGU4ZWRkMGI3LWQ0NDctNGE3ZS1iN2QyLTlmODg2YmFlMzMyNwBGAAAAAABVVKFeqdKSSYHCm2TtRWO0BwA1Qe0bgT4cTptlxQT1mdz3AAAAAAEMAAA1Qe0bgT4cTptlxQT1mdz3AAAG-Q3aAAA=/
AAMkAGU4ZWRkMGI3LWQ0NDctNGE3ZS1iN2QyLTlmODg2YmFlMzMyNwBGAAAAAABVVKFeqdKSSYHCm2TtRWO0BwA1Qe0bgT4cTptlxQT1mdz3AAAAAAEMAAA1Qe0bgT4cTptlxQT1mdz3AAAG-Q3aAAA=/body.HTML
Folder AAMkAGU4ZWRkMGI3LWQ0NDctNGE3ZS1iN2QyLTlmODg2YmFlMzMyNwBGAAAAAABVVKFeqdKSSYHCm2TtRWO0BwA1Qe0bgT4cTptlxQT1mdz3AAAAAAEMAAA1Qe0bgT4cTptlxQT1mdz3AAAG-Q3ZAAA= created in Bucket
Uploaded email content as /tmp/body.HTML to S3
Uploaded /tmp/Inflationary Trend Selection for Modeling_2024-05-07.xlsx to S3
AAMkAGU4ZWRkMGI3LWQ0NDctNGE3ZS1iN2QyLTlmODg2YmFlMzMyNwBGAAAAAABVVKFeqdKSSYHCm2TtRWO0BwA1Qe0bgT4cTptlxQT1mdz3AAAAAAEMAAA1Qe0bgT4cTptlxQT1mdz3AAAG-Q3ZAAA=/
AAMkAGU4ZWRkMGI3LWQ0NDctNGE3ZS1iN2QyLTlmODg2YmFlMzMyNwBGAAAAAABVVKFeqdKSSYHCm2TtRWO0BwA1Qe0bgT4cTptlxQT1mdz3AAAAAAEMAAA1Qe0bgT4cTptlxQT1mdz3AAAG-Q3ZAAA=/Inflationary Trend Selection for Modeling_2024-05-07.xlsx
AAMkAGU4ZWRkMGI3LWQ0NDctNGE3ZS1iN2QyLTlmODg2YmFlMzMyNwBGAAAAAABVVKFeqdKSSYHCm2TtRWO0BwA1Qe0bgT4cTptlxQT1mdz3AAAAAAEMAAA1Qe0bgT4cTptlxQT1mdz3AAAG-Q3ZAAA=/body.HTML
Folder AAMkAGU4ZWRkMGI3LWQ0NDctNGE3ZS1iN2QyLTlmODg2YmFlMzMyNwBGAAAAAABVVKFeqdKSSYHCm2TtRWO0BwA1Qe0bgT4cTptlxQT1mdz3AAAAAAEMAAA1Qe0bgT4cTptlxQT1mdz3AAAG-Q3YAAA= created in Bucket
Uploaded email content as /tmp/body.HTML to S3
RequestId: 022cd06e-8ef4-4cc9-93ad-6c1f0cfae97f Error: Runtime exited with error: signal: killed
Runtime.ExitError
END RequestId: 022cd06e-8ef4-4cc9-93ad-6c1f0cfae97f
REPORT RequestId: 022cd06e-8ef4-4cc9-93ad-6c1f0cfae97f	Duration: 14427.65 ms	Billed Duration: 14428 ms	Memory Size: 128 MB	Max Memory Used: 128 MB

Request ID
022cd06e-8ef4-4cc9-93ad-6c1f0cfae97f
